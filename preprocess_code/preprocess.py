# -*- coding: utf-8 -*-
"""Preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S0fv8cfnHEjn7PzG7W4XjXjYvLIeIAND
"""

from tqdm import tqdm
from PIL import Image

import random
from datetime import datetime
import cv2
import numpy as np
import math
import tensorflow as tf
import csv
import matplotlib.pyplot as plt
import tensorflow_models as tfm

"""Datasplit:

1: TRAIN: ['H', 'X', 'L', 'Y'] TEST: ['G']

2: TRAIN: ['H', 'G', 'L', 'Y'] TEST: ['X']

3: TRAIN: ['H', 'X', 'G'] TEST: ['L', 'Y']
"""

'''
adding salt & pepper noise to dataset
CREDIT: Wang et al. HUST-OBS data preprocessing

NOTE: In order to create comparable results, we need to
do the same preprocessing steps on the dataset
'''
def salt_and_pepper_noise(image):
  if np.random.random() < 0.5:
      image1 = np.array(image)

      # add noise
      salt_vs_pepper_ratio = np.random.uniform(0, 0.4)
      amount = np.random.uniform(0, 0.006)
      num_salt = np.ceil(amount * image1.size / 3 * salt_vs_pepper_ratio)
      num_pepper = np.ceil(amount * image1.size / 3 * (1.0 - salt_vs_pepper_ratio))

      # Generate at random locations
      coords_salt = [np.random.randint(0, i - 1, int(num_salt)) for i in image1.shape]
      coords_pepper = [np.random.randint(0, i - 1, int(num_pepper)) for i in image1.shape]

      # image1[coords_salt] = 255
      image1[coords_salt[0], coords_salt[1], :] = 255
      image1[coords_pepper[0], coords_pepper[1], :] = 0
      image = Image.fromarray(image1)

  return image

'''
erode_and_dialate image for noise removal, segmentation, and feature extraction
CREDIT: Wang et al. HUST-OBS data preprocessing

NOTE: In order to create comparable results, we need to
do the same preprocessing steps on the dataset
'''
def erode_and_dialate(image):
  # Generate a random number between 0 and 2
  random_value = random.random() * 3

  if random_value < 1:  # 1/3 probability of performing addition operation
      he = random.randint(1, 3)
      kernel = np.ones((he, he), np.uint8)
      image = cv2.erode(image, kernel, iterations=1)
  elif random_value < 2:  # 1/3 probability of performing division operation
      he = random.randint(1, 3)  # Generate a random integer between 1 and 10 as the divisor
      kernel = np.ones((he, he), np.uint8)
      image = cv2.dilate(image, kernel, iterations=1)
  return image

class RandomGaussianBlur(object):
    def __init__(self, p=0.5, min_kernel_size=3, max_kernel_size=15, min_sigma=0.1, max_sigma=1.0):
        self.p = p
        self.min_kernel_size = min_kernel_size
        self.max_kernel_size = max_kernel_size
        self.min_sigma = min_sigma
        self.max_sigma = max_sigma

    def __call__(self, img):
        if random.random() < self.p and self.min_kernel_size < self.max_kernel_size:
            kernel_size = random.randrange(self.min_kernel_size, self.max_kernel_size + 1, 2)
            sigma = random.uniform(self.min_sigma, self.max_sigma)
            return tfm.vision.augment.gaussian_filter2d(img, kernel_size, sigma)
        else:
            return img

def pad_with_white(image, xl, yl, xr, yr):
  # white is RGB (255, 255, 255), so we want to pad with 255
  # the dimensions of the image are (height, width, channels)
  paddings = [[yl, yr], [xl, xr], [0, 0]]
  image = tf.pad(image, paddings, mode='CONSTANT', constant_values=255)
  return image

class ColorJitter(object):
  def __init__(self, brightdelta, contrastdelta, satdelta, huedelta):
    self.brightdelta = brightdelta
    self.contrastdelta = contrastdelta
    self.satdelta = satdelta
    self.huedelta = huedelta

  def __call__(self, image):
    image = tf.image.random_brightness(image, self.brightdelta)
    image = tf.image.random_contrast(image, max(0, 1 - self.contrastdelta), 1 + self.contrastdelta)
    image = tf.image.random_saturation(image, max(0, 1 - self.satdelta), 1 + self.satdelta)
    image = tf.image.random_hue(image, self.huedelta)
    return image

def random_apply(image, transforms, p):
  if random.random() <= p:
    for transform in transforms:
      image = transform(image)
  return image

def tensorflow_normalize(image, mean, std):
    image = tf.convert_to_tensor(image, dtype=tf.float32)
    image = (image - mean) / std
    return image

'''
process_image_train converts a png to a tensor
'''
def process_image_train(image):
  # Checks if image is grayscale and converts to RGB
  if image.mode == 'L':
        image = image.convert('RGB')

  # Resize image to 72 x 72
  w, h = image.size
  if w > h:
      x = 72
      y = round(h / w * 72)
  # x, y = 72,72
  else:
      y = 72
      x = round(w / h * 72)

  # Reshape to desired x & y of 129 x 129 pixels
  sizey, sizex = 129, 129
  if y < 128:
      while sizey > 128 or sizey < 16:
          sizey = round(random.gauss(y, 30))
  if x < 128:
      while sizex > 128 or sizex < 16:
          sizex = round(random.gauss(x, 30))

  dx = 128 - sizex
  dy = 128 - sizey

  if dx > 0:
      xl = -1
      while xl > dx or xl < 0:
          xl = round(dx / 2)
          xl = round(random.gauss(xl, 10))
  else:
      xl = 0
  if dy > 0:
      yl = -1
      while yl > dy or yl < 0:
          yl = round(dy / 2)
          yl = round(random.gauss(yl, 10))
  else:
      yl = 0

  yr = dy - yl
  xr = dx - xl

  # Image processing
  image = salt_and_pepper_noise(image)
  image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
  image = erode_and_dialate(image)
  image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
  random_gaussian_blur = RandomGaussianBlur()
  image = random_gaussian_blur(image)
  image = tf.image.resize(image, (sizey, sizex))
  image = tf.image.random_flip_left_right(image)
  image = pad_with_white(image, xl, yl, xr, yr)
  image = tf.keras.layers.RandomRotation(factor=15/(180 * math.pi), fill_mode='constant', fill_value=255)(image)
  mean = [0.85233593, 0.85246795, 0.8517555]
  std = [0.31232414, 0.3122127, 0.31273854]
  image = tensorflow_normalize(image, mean, std)
  return image

'''
process_image_test converts a png to a tensor
'''
def process_image_test(image):
  # Checks if image is grayscale and converts to RGB
  if image.mode == 'L':
        image = image.convert('RGB')

  # Resize image to 72 x 72
  w, h = image.size
  if w > h:
      dy = w - h
      yl = round(dy / 2)
      yr = dy - yl
      image = pad_with_white(image, 0, yl, 0, yr)
  else:
      dx = h - w
      xl = round(dx / 2)
      xr = dx - xl
      image = pad_with_white(image, xl, 0, xr, 0)

  image = tf.image.resize(image, (128, 128))
  mean = [0.85233593, 0.85246795, 0.8517555]
  std = [0.31232414, 0.3122127, 0.31273854]
  image = tensorflow_normalize(image, mean, std)
  return image

from tqdm import tqdm

def preprocess(path: str, process_type: str):
    assert process_type in ["train", "test"] # check if preprocess type is train/test

    with open(path, newline='') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')

        # process labels
        inputs = []
        labels = []

        # Count the total number of rows excluding the header
        total_rows = sum(1 for _ in reader) - 1  # Exclude the header row
        csvfile.seek(0)  # Reset the file pointer

        # process images
        header = True
        for row in tqdm(reader, total=total_rows, desc=f'Processing {process_type} data'):
            if header is True:
                header = False
            else:
                label = int(row[0]) # get labels
                image_path = "../" + row[1] # get images
                image = Image.open(image_path) # open image

                if (process_type=="train"):
                    image = process_image_train(image) # process image for train

                if (process_type == "test"):
                    image = process_image_test(image) # process image for test

                # update the labels & images lists
                labels.append(label) # add label to label list
                inputs.append(image)

    labels = tf.convert_to_tensor(labels)
    print(labels.shape)

    # labels = tf.convert_to_tensor(np.array(labels))
    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
    return dataset

# train or test
process_type = {
    "mock": "train",
    "wangtest": "test",
    "wangtrain": "train",
    "train1": "train",
    "train2": "train",
    "train3": "train",
    "test1": "test",
    "test2": "test",
    "test3": "test"
}

# Storing file paths to train & test csv datasets:
file_paths = {
    "mock": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/mock.csv",
    "wangtest": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/wang_test.csv",
    "wangtrain":"/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/wang_train.csv",
    "train1": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/train1-HXLY.csv",
    "train2": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/train2-HGLY.csv",
    "train3": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/train3-HXG.csv",
    "test1": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/test1-G.csv",
    "test2": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/test2-X.csv",
    "test3": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/DATA/test3-LY.csv"
}

saved_data_paths = {
    "mock": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/mock",
    "wangtest": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/wangtest",
    "wangtrain": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/wangtrain",
    "train1":"/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/train1",
    "train2": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/train2",
    "train3": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/train3",
    "test1": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/test1",
    "test2": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/test2",
    "test3": "/Users/michelleding/Desktop/oracle/CSCI1470-Final-Oracle/preprocess_code/saved_tf/test3"
}

# preprocess takes in
# 1) file path (see file_paths dict)
# 2) "train" or "test" mode of preprocess
key = "mock"
print("==========================")
print(key)
print("--------------------------")
dataset = preprocess(file_paths[key], process_type[key])
save_path = saved_data_paths[key]
try:
    dataset.save(save_path)
    print("Dataset", key, "saved successfully")
except Exception as e:
    print("Error saving dataset:", e)

# # preprocess takes in
# # 1) file path (see file_paths dict)
# # 2) "train" or "test" mode of preprocess

# for key in file_paths.keys():
#   print("==========================")
#   print(key)
#   print("--------------------------")
#   dataset = preprocess(file_paths[key], process_type[key])
#   save_path = saved_data_paths[key]
#   try:
#       dataset.save(save_path)
#       print("Dataset", key, "saved successfully")
#   except Exception as e:
#       print("Error saving dataset:", e)

# print("All datasets saved")
